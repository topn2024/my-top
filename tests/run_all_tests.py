#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»æµ‹è¯•è¿è¡Œå™¨
æä¾›ç»Ÿä¸€çš„æµ‹è¯•æ‰§è¡Œå…¥å£å’ŒæŠ¥å‘Šç”Ÿæˆ
"""
import sys
import os
import time
import argparse
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tests.utils.test_helpers import TestTimer, save_test_result
from tests.config.test_config import get_config


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨ç±»"""

    def __init__(self, config=None):
        """
        åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨

        Args:
            config: æµ‹è¯•é…ç½®
        """
        self.config = config or get_config()
        self.project_root = project_root
        self.tests_dir = self.project_root / 'tests'
        self.results = []

    def run_all_tests(self, test_type: str = None, coverage: bool = None, parallel: bool = None):
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•

        Args:
            test_type: æµ‹è¯•ç±»å‹ (unit, integration, api, ui, e2e)
            coverage: æ˜¯å¦ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
            parallel: æ˜¯å¦å¹¶è¡Œæ‰§è¡Œ
        """
        print("ğŸ§ª TOP_N è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶")
        print("=" * 70)

        # ä½¿ç”¨é…ç½®å€¼æˆ–å‚æ•°å€¼
        coverage = coverage if coverage is not None else self.config.get('test.coverage', False)
        parallel = parallel if parallel is not None else self.config.get('test.parallel', False)

        # ç¡®å®šæµ‹è¯•ç±»å‹
        test_types = self._get_test_types(test_type)

        print(f"ğŸ“‹ æµ‹è¯•ç±»å‹: {', '.join(test_types)}")
        print(f"ğŸ“Š ç”Ÿæˆè¦†ç›–ç‡: {'æ˜¯' if coverage else 'å¦'}")
        print(f"âš¡ å¹¶è¡Œæ‰§è¡Œ: {'æ˜¯' if parallel else 'å¦'}")
        print()

        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        self._setup_report_dirs()

        # è¿è¡Œæµ‹è¯•å¥—ä»¶
        total_start_time = time.time()
        overall_success = True

        for test_type_name in test_types:
            print(f"\nğŸ” è¿è¡Œ {test_type_name.upper()} æµ‹è¯•å¥—ä»¶...")
            print("-" * 50)

            with TestTimer(f"{test_type_name.upper()} æµ‹è¯•"):
                success = self._run_test_suite(test_type_name, coverage, parallel)
                if not success:
                    overall_success = False

        # ç”Ÿæˆæ€»æŠ¥å‘Š
        total_duration = time.time() - total_start_time
        self._generate_summary_report(overall_success, total_duration, test_types)

        return overall_success

    def _get_test_types(self, test_type: Optional[str]) -> List[str]:
        """è·å–æµ‹è¯•ç±»å‹åˆ—è¡¨"""
        all_types = ['unit', 'integration', 'api', 'ui', 'e2e']

        if test_type is None:
            return all_types
        elif test_type == 'all':
            return all_types
        elif test_type in all_types:
            return [test_type]
        else:
            raise ValueError(f"æ— æ•ˆçš„æµ‹è¯•ç±»å‹: {test_type}. æ”¯æŒçš„ç±»å‹: {all_types}")

    def _setup_report_dirs(self):
        """è®¾ç½®æŠ¥å‘Šç›®å½•"""
        reports_dir = self.config.get('paths.reports_dir')
        Path(reports_dir).mkdir(exist_ok=True)

        # æ¸…ç†æ—§çš„HTMLæŠ¥å‘Š
        for file in Path(reports_dir).glob("*.html"):
            if not file.name.startswith("report_"):
                file.unlink()

    def _run_test_suite(self, test_type: str, coverage: bool, parallel: bool) -> bool:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        test_dir = self.tests_dir / test_type

        if not test_dir.exists():
            print(f"âš ï¸ æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_dir}")
            return True

        # æ„å»ºpytestå‘½ä»¤
        cmd = self._build_pytest_command(test_type, test_dir, coverage, parallel)

        try:
            # æ‰§è¡Œæµ‹è¯•
            result = subprocess.run(
                cmd,
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=self.config.get('test.timeout', 300)
            )

            # è®°å½•ç»“æœ
            suite_result = {
                'type': test_type,
                'command': ' '.join(cmd),
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'duration': 0  # å°†åœ¨TestTimerä¸­è®¾ç½®
            }

            self.results.append(suite_result)

            # æ˜¾ç¤ºç»“æœ
            self._display_suite_result(test_type, suite_result)

            return result.returncode == 0

        except subprocess.TimeoutExpired:
            print(f"âŒ {test_type.upper()} æµ‹è¯•è¶…æ—¶")
            return False
        except Exception as e:
            print(f"âŒ {test_type.upper()} æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
            return False

    def _build_pytest_command(self, test_type: str, test_dir: Path, coverage: bool, parallel: bool) -> List[str]:
        """æ„å»ºpytestå‘½ä»¤"""
        cmd = [sys.executable, "-m", "pytest"]

        # æµ‹è¯•ç›®å½•
        cmd.append(str(test_dir))

        # è¯¦ç»†è¾“å‡º
        cmd.append("-v")

        # è¾“å‡ºæ ¼å¼
        cmd.extend(["--tb=short", "--strict-markers"])

        # HTMLæŠ¥å‘Š
        reports_dir = self.config.get('paths.reports_dir')
        html_report = f"{reports_dir}/report_{test_type}.html"
        cmd.extend(["--html", html_report])

        # JSONæŠ¥å‘Š
        json_report = f"{reports_dir}/report_{test_type}.json"
        cmd.extend(["--json-report", "--json-report-file", json_report])

        # è¦†ç›–ç‡
        if coverage:
            cmd.extend(["--cov=" + self._get_coverage_package(test_type)])
            cmd.extend(["--cov-report", f"html:{reports_dir}/coverage_{test_type}"])
            cmd.extend(["--cov-report", f"xml:{reports_dir}/coverage_{test_type}.xml"])
            cmd.extend(["--cov-report", f"term-missing"])

        # å¹¶è¡Œæ‰§è¡Œ
        if parallel:
            import multiprocessing
            workers = min(multiprocessing.cpu_count(), 4)
            cmd.extend(["-n", str(workers)])

        # æ ‡è®°è¿‡æ»¤
        if test_type == 'ui':
            cmd.append("-m ui")
        elif test_type == 'api':
            cmd.append("-m api")
        elif test_type == 'integration':
            cmd.append("-m integration")

        return cmd

    def _get_coverage_package(self, test_type: str) -> str:
        """è·å–è¦†ç›–ç‡åŒ…å"""
        coverage_mapping = {
            'unit': 'backend',
            'integration': 'backend',
            'api': 'backend',
            'ui': None,  # UIæµ‹è¯•é€šå¸¸ä¸éœ€è¦è¦†ç›–ç‡
            'e2e': None
        }
        return coverage_mapping.get(test_type)

    def _display_suite_result(self, test_type: str, result: Dict[str, Any]):
        """æ˜¾ç¤ºæµ‹è¯•å¥—ä»¶ç»“æœ"""
        if result['return_code'] == 0:
            print(f"âœ… {test_type.upper()} æµ‹è¯•é€šè¿‡")
        else:
            print(f"âŒ {test_type.upper()} æµ‹è¯•å¤±è´¥")

        # æ˜¾ç¤ºè¾“å‡ºï¼ˆç®€åŒ–ç‰ˆï¼‰
        stdout_lines = result['stdout'].split('\n')
        for line in stdout_lines:
            if 'passed' in line and 'failed' in line:
                print(f"ğŸ“Š {line.strip()}")
                break

        if result['stderr']:
            error_lines = result['stderr'].split('\n')[:5]  # åªæ˜¾ç¤ºå‰5è¡Œé”™è¯¯
            for line in error_lines:
                if line.strip():
                    print(f"âš ï¸ {line}")

        print()

    def _generate_summary_report(self, overall_success: bool, total_duration: float, test_types: List[str]):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("=" * 70)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 70)

        # ç»Ÿè®¡ä¿¡æ¯
        total_suites = len(test_types)
        passed_suites = sum(1 for r in self.results if r['return_code'] == 0)
        failed_suites = total_suites - passed_suites

        print(f"ğŸ§ª æµ‹è¯•å¥—ä»¶æ€»æ•°: {total_suites}")
        print(f"âœ… é€šè¿‡å¥—ä»¶: {passed_suites}")
        print(f"âŒ å¤±è´¥å¥—ä»¶: {failed_suites}")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’")

        # è¯¦ç»†ç»“æœ
        print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in self.results:
            status = "âœ… é€šè¿‡" if result['return_code'] == 0 else "âŒ å¤±è´¥"
            print(f"  {result['type'].upper():10} : {status}")

        # æŠ¥å‘Šä½ç½®
        reports_dir = self.config.get('paths.reports_dir')
        print(f"\nğŸ“ æŠ¥å‘Šä½ç½®: {reports_dir}")

        # HTMLæŠ¥å‘Šé“¾æ¥
        html_files = list(Path(reports_dir).glob("report_*.html"))
        if html_files:
            print("ğŸ“„ HTMLæŠ¥å‘Š:")
            for html_file in html_files:
                print(f"  - {html_file}")

        # ä¿å­˜ç»“æœ
        summary_result = {
            'overall_success': overall_success,
            'total_suites': total_suites,
            'passed_suites': passed_suites,
            'failed_suites': failed_suites,
            'total_duration': total_duration,
            'test_types': test_types,
            'results': self.results,
            'timestamp': time.time()
        }

        save_test_result('test_suite_summary', summary_result)

        # æœ€ç»ˆçŠ¶æ€
        print("\n" + "=" * 70)
        if overall_success:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¥åº·çŠ¶æ€è‰¯å¥½ã€‚")
        else:
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¯¦ç»†æŠ¥å‘Šã€‚")
        print("=" * 70)

    def run_quick_tests(self):
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•"""
        print("âš¡ è¿è¡Œå¿«é€Ÿæµ‹è¯•å¥—ä»¶...")
        return self.run_all_tests(test_type='unit', coverage=False, parallel=False)

    def run_full_tests(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸ”¬ è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
        return self.run_all_tests(test_type='all', coverage=True, parallel=True)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TOP_N æµ‹è¯•è¿è¡Œå™¨')

    parser.add_argument('--type', '-t',
                      choices=['unit', 'integration', 'api', 'ui', 'e2e', 'all'],
                      default='all',
                      help='æµ‹è¯•ç±»å‹')

    parser.add_argument('--coverage', '-c', action='store_true',
                      help='ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š')

    parser.add_argument('--no-coverage', dest='no_coverage', action='store_true',
                      help='ä¸ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š')

    parser.add_argument('--parallel', '-p', action='store_true',
                      help='å¹¶è¡Œæ‰§è¡Œæµ‹è¯•')

    parser.add_argument('--no-parallel', dest='no_parallel', action='store_true',
                      help='ä¸å¹¶è¡Œæ‰§è¡Œæµ‹è¯•')

    parser.add_argument('--quick', '-q', action='store_true',
                      help='è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆä»…å•å…ƒæµ‹è¯•ï¼‰')

    parser.add_argument('--full', '-f', action='store_true',
                      help='è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶')

    parser.add_argument('--environment', '-e',
                      choices=['development', 'staging', 'production'],
                      help='æµ‹è¯•ç¯å¢ƒ')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = get_config(args.environment)

    # ç¡®å®šå‚æ•°å€¼
    coverage = config.get('test.coverage', False)
    parallel = config.get('test.parallel', False)

    if args.coverage:
        coverage = True
    elif args.no_coverage:
        coverage = False

    if args.parallel:
        parallel = True
    elif args.no_parallel:
        parallel = False

    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = TestRunner(config)

    try:
        # æ‰§è¡Œæµ‹è¯•
        if args.quick:
            success = runner.run_quick_tests()
        elif args.full:
            success = runner.run_full_tests()
        else:
            success = runner.run_all_tests(
                test_type=args.type,
                coverage=coverage,
                parallel=parallel
            )

        # è®¾ç½®é€€å‡ºç 
        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå™¨å¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()