#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—åˆ†æå·¥å…·
æä¾›æ—¥å¿—æŸ¥è¯¢ã€åˆ†æã€ç»Ÿè®¡åŠŸèƒ½
"""
import os
import sys
import re
import argparse
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import json

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

LOG_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs')


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""

    def __init__(self, log_dir=LOG_DIR):
        self.log_dir = log_dir

    def tail_logs(self, log_file='all.log', lines=50, follow=False):
        """
        æŸ¥çœ‹æ—¥å¿—å°¾éƒ¨

        Args:
            log_file: æ—¥å¿—æ–‡ä»¶å
            lines: æ˜¾ç¤ºè¡Œæ•°
            follow: æ˜¯å¦å®æ—¶è·Ÿè¸ª
        """
        log_path = os.path.join(self.log_dir, log_file)

        if not os.path.exists(log_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            return

        if follow:
            # å®æ—¶è·Ÿè¸ªæ—¥å¿—
            import subprocess
            if sys.platform == 'win32':
                # Windowsä½¿ç”¨PowerShell
                cmd = f'powershell Get-Content -Path "{log_path}" -Wait -Tail {lines}'
            else:
                # Linux/Macä½¿ç”¨tail -f
                cmd = f'tail -f -n {lines} "{log_path}"'

            try:
                subprocess.run(cmd, shell=True)
            except KeyboardInterrupt:
                print("\nå·²åœæ­¢è·Ÿè¸ªæ—¥å¿—")
        else:
            # æ˜¾ç¤ºæœ€åNè¡Œ
            try:
                with open(log_path, 'r', encoding='utf-8') as f:
                    all_lines = f.readlines()
                    last_lines = all_lines[-lines:]
                    print(f"\n{'=' * 100}")
                    print(f"ğŸ“„ {log_file} - æœ€å {lines} è¡Œ")
                    print(f"{'=' * 100}\n")
                    for line in last_lines:
                        print(line.rstrip())
            except Exception as e:
                print(f"âŒ è¯»å–æ—¥å¿—å¤±è´¥: {e}")

    def search_logs(self, keyword, log_file='all.log', case_sensitive=False, context=0):
        """
        æœç´¢æ—¥å¿—å†…å®¹

        Args:
            keyword: æœç´¢å…³é”®è¯
            log_file: æ—¥å¿—æ–‡ä»¶å
            case_sensitive: æ˜¯å¦åŒºåˆ†å¤§å°å†™
            context: æ˜¾ç¤ºä¸Šä¸‹æ–‡è¡Œæ•°
        """
        log_path = os.path.join(self.log_dir, log_file)

        if not os.path.exists(log_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            return

        print(f"\n{'=' * 100}")
        print(f"ğŸ” æœç´¢å…³é”®è¯: '{keyword}' (æ–‡ä»¶: {log_file})")
        print(f"{'=' * 100}\n")

        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            matches = []
            for i, line in enumerate(lines):
                search_line = line if case_sensitive else line.lower()
                search_keyword = keyword if case_sensitive else keyword.lower()

                if search_keyword in search_line:
                    matches.append((i, line))

            if matches:
                print(f"âœ“ æ‰¾åˆ° {len(matches)} å¤„åŒ¹é…:\n")
                for line_num, line in matches:
                    # æ˜¾ç¤ºä¸Šä¸‹æ–‡
                    if context > 0:
                        start = max(0, line_num - context)
                        end = min(len(lines), line_num + context + 1)
                        print(f"--- è¡Œ {line_num + 1} ---")
                        for i in range(start, end):
                            marker = ">>> " if i == line_num else "    "
                            print(f"{marker}{lines[i].rstrip()}")
                        print()
                    else:
                        print(f"è¡Œ {line_num + 1}: {line.rstrip()}")
            else:
                print("âŒ æœªæ‰¾åˆ°åŒ¹é…é¡¹")

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")

    def search_by_request_id(self, request_id, log_file='all.log'):
        """
        æ ¹æ®è¯·æ±‚IDæœç´¢å®Œæ•´è¯·æ±‚é“¾è·¯

        Args:
            request_id: è¯·æ±‚ID
            log_file: æ—¥å¿—æ–‡ä»¶å
        """
        log_path = os.path.join(self.log_dir, log_file)

        if not os.path.exists(log_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            return

        print(f"\n{'=' * 100}")
        print(f"ğŸ”— è¿½è¸ªè¯·æ±‚: {request_id}")
        print(f"{'=' * 100}\n")

        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            matches = []
            for i, line in enumerate(lines):
                if request_id in line:
                    matches.append((i, line))

            if matches:
                print(f"âœ“ æ‰¾åˆ° {len(matches)} æ¡ç›¸å…³æ—¥å¿—:\n")
                for line_num, line in matches:
                    print(line.rstrip())
            else:
                print(f"âŒ æœªæ‰¾åˆ°è¯·æ±‚ID: {request_id}")

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")

    def analyze_errors(self, log_file='error.log', hours=24):
        """
        åˆ†æé”™è¯¯æ—¥å¿—

        Args:
            log_file: æ—¥å¿—æ–‡ä»¶å
            hours: åˆ†ææœ€è¿‘Nå°æ—¶çš„æ—¥å¿—
        """
        log_path = os.path.join(self.log_dir, log_file)

        if not os.path.exists(log_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            return

        print(f"\n{'=' * 100}")
        print(f"ğŸ“Š é”™è¯¯æ—¥å¿—åˆ†æ - æœ€è¿‘ {hours} å°æ—¶")
        print(f"{'=' * 100}\n")

        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            error_types = Counter()
            error_details = []

            with open(log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    # è§£ææ—¶é—´æˆ³
                    match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                    if match:
                        log_time_str = match.group(1)
                        try:
                            log_time = datetime.strptime(log_time_str, '%Y-%m-%d %H:%M:%S')
                            if log_time < cutoff_time:
                                continue
                        except:
                            pass

                    # æå–é”™è¯¯ç±»å‹
                    if 'ERROR' in line:
                        # å°è¯•æå–å¼‚å¸¸ç±»å‹
                        exc_match = re.search(r'([\w]+Error|[\w]+Exception):', line)
                        if exc_match:
                            error_type = exc_match.group(1)
                            error_types[error_type] += 1
                            error_details.append({
                                'type': error_type,
                                'message': line.strip()[:200]
                            })

            # æ˜¾ç¤ºç»Ÿè®¡
            print(f"é”™è¯¯æ€»æ•°: {sum(error_types.values())}\n")

            if error_types:
                print("é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
                for error_type, count in error_types.most_common(10):
                    percentage = (count / sum(error_types.values())) * 100
                    bar = 'â–ˆ' * int(percentage / 2)
                    print(f"  {error_type:30} | {count:4} | {percentage:5.1f}% {bar}")

                print(f"\næœ€è¿‘çš„é”™è¯¯:")
                for detail in error_details[-5:]:
                    print(f"  [{detail['type']}] {detail['message']}")
            else:
                print("âœ“ æœ€è¿‘æ²¡æœ‰é”™è¯¯")

        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")

    def analyze_performance(self, log_file='performance.log', hours=24):
        """
        åˆ†ææ€§èƒ½æ—¥å¿—

        Args:
            log_file: æ—¥å¿—æ–‡ä»¶å
            hours: åˆ†ææœ€è¿‘Nå°æ—¶çš„æ—¥å¿—
        """
        log_path = os.path.join(self.log_dir, log_file)

        if not os.path.exists(log_path):
            print(f"âš ï¸  æ€§èƒ½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            print("   æ€§èƒ½æ—¥å¿—éœ€è¦åœ¨ä½¿ç”¨å¢å¼ºç‰ˆlogger_configåæ‰ä¼šç”Ÿæˆ")
            return

        print(f"\n{'=' * 100}")
        print(f"âš¡ æ€§èƒ½åˆ†æ - æœ€è¿‘ {hours} å°æ—¶")
        print(f"{'=' * 100}\n")

        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            operations = defaultdict(list)

            with open(log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    # è§£ææ—¶é—´æˆ³
                    match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                    if match:
                        log_time_str = match.group(1)
                        try:
                            log_time = datetime.strptime(log_time_str, '%Y-%m-%d %H:%M:%S')
                            if log_time < cutoff_time:
                                continue
                        except:
                            pass

                    # è§£ææ“ä½œå’Œè€—æ—¶
                    # æ ¼å¼: timestamp | request_id | operation | duration | message
                    parts = line.split('|')
                    if len(parts) >= 4:
                        operation = parts[2].strip()
                        duration_str = parts[3].strip()
                        # æå–æ•°å­—
                        duration_match = re.search(r'([\d.]+)s', duration_str)
                        if duration_match:
                            duration = float(duration_match.group(1))
                            operations[operation].append(duration)

            if operations:
                print(f"æ€»è¯·æ±‚æ•°: {sum(len(v) for v in operations.values())}\n")
                print("æ“ä½œæ€§èƒ½ç»Ÿè®¡:")
                print(f"{'æ“ä½œ':40} | {'æ¬¡æ•°':6} | {'å¹³å‡':8} | {'æœ€å°':8} | {'æœ€å¤§':8} | {'P95':8}")
                print("-" * 100)

                for operation, durations in sorted(operations.items(), key=lambda x: sum(x[1])/len(x[1]), reverse=True)[:20]:
                    count = len(durations)
                    avg = sum(durations) / count
                    min_time = min(durations)
                    max_time = max(durations)
                    sorted_durations = sorted(durations)
                    p95 = sorted_durations[int(len(sorted_durations) * 0.95)] if count > 1 else durations[0]

                    print(f"{operation[:40]:40} | {count:6} | {avg:7.3f}s | {min_time:7.3f}s | {max_time:7.3f}s | {p95:7.3f}s")
            else:
                print("âš ï¸  æ²¡æœ‰æ€§èƒ½æ•°æ®")

        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")

    def analyze_slow_queries(self, log_file='slow.log'):
        """
        åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—

        Args:
            log_file: æ—¥å¿—æ–‡ä»¶å
        """
        log_path = os.path.join(self.log_dir, log_file)

        if not os.path.exists(log_path):
            print(f"âš ï¸  æ…¢æŸ¥è¯¢æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            print("   æ…¢æŸ¥è¯¢æ—¥å¿—éœ€è¦åœ¨ä½¿ç”¨å¢å¼ºç‰ˆlogger_configåæ‰ä¼šç”Ÿæˆ")
            return

        print(f"\n{'=' * 100}")
        print(f"ğŸŒ æ…¢æŸ¥è¯¢åˆ†æ")
        print(f"{'=' * 100}\n")

        try:
            slow_queries = []

            with open(log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    # è§£ææ…¢æŸ¥è¯¢ä¿¡æ¯
                    if 'SLOW' in line:
                        # æå–æ“ä½œåå’Œè€—æ—¶
                        match = re.search(r'SLOW (API|QUERY|SERVICE): (.+?) took ([\d.]+)s', line)
                        if match:
                            query_type = match.group(1)
                            operation = match.group(2)
                            duration = float(match.group(3))
                            slow_queries.append({
                                'type': query_type,
                                'operation': operation,
                                'duration': duration,
                                'line': line.strip()
                            })

            if slow_queries:
                print(f"æ…¢æŸ¥è¯¢æ€»æ•°: {len(slow_queries)}\n")

                # æŒ‰ç±»å‹åˆ†ç»„
                by_type = defaultdict(list)
                for query in slow_queries:
                    by_type[query['type']].append(query)

                for query_type, queries in by_type.items():
                    print(f"\n{query_type} ç±»å‹æ…¢æŸ¥è¯¢ ({len(queries)} ä¸ª):")
                    print("-" * 100)
                    for query in sorted(queries, key=lambda x: x['duration'], reverse=True)[:10]:
                        print(f"  {query['duration']:6.2f}s | {query['operation']}")
            else:
                print("âœ“ æ²¡æœ‰æ…¢æŸ¥è¯¢")

        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")

    def list_log_files(self):
        """åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        print(f"\n{'=' * 100}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶åˆ—è¡¨ ({self.log_dir})")
        print(f"{'=' * 100}\n")

        try:
            if not os.path.exists(self.log_dir):
                print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_dir}")
                return

            log_files = []
            for file in os.listdir(self.log_dir):
                if file.endswith('.log'):
                    file_path = os.path.join(self.log_dir, file)
                    size = os.path.getsize(file_path)
                    mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    log_files.append((file, size, mtime))

            if log_files:
                print(f"{'æ–‡ä»¶å':30} | {'å¤§å°':12} | {'æœ€åä¿®æ”¹æ—¶é—´':20}")
                print("-" * 100)
                for file, size, mtime in sorted(log_files, key=lambda x: x[2], reverse=True):
                    size_str = self._format_size(size)
                    mtime_str = mtime.strftime('%Y-%m-%d %H:%M:%S')
                    print(f"{file:30} | {size_str:12} | {mtime_str:20}")
            else:
                print("âŒ æ²¡æœ‰æ—¥å¿—æ–‡ä»¶")

        except Exception as e:
            print(f"âŒ åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")

    @staticmethod
    def _format_size(size):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024:
                return f"{size:.1f} {unit}"
            size /= 1024
        return f"{size:.1f} TB"


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='TOP_N æ—¥å¿—åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹æœ€å50è¡Œæ—¥å¿—
  python log_analyzer.py --tail

  # å®æ—¶è·Ÿè¸ªæ—¥å¿—
  python log_analyzer.py --tail --follow

  # æœç´¢å…³é”®è¯
  python log_analyzer.py --search "ERROR"

  # æ ¹æ®è¯·æ±‚IDè¿½è¸ª
  python log_analyzer.py --request-id abc12345

  # åˆ†æé”™è¯¯
  python log_analyzer.py --analyze-errors

  # åˆ†ææ€§èƒ½
  python log_analyzer.py --analyze-performance

  # åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶
  python log_analyzer.py --list
        """
    )

    parser.add_argument('--tail', action='store_true', help='æŸ¥çœ‹æ—¥å¿—å°¾éƒ¨')
    parser.add_argument('--follow', '-f', action='store_true', help='å®æ—¶è·Ÿè¸ªæ—¥å¿—')
    parser.add_argument('--lines', '-n', type=int, default=50, help='æ˜¾ç¤ºè¡Œæ•°ï¼ˆé»˜è®¤50ï¼‰')
    parser.add_argument('--search', '-s', metavar='KEYWORD', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--request-id', '-r', metavar='ID', help='æ ¹æ®è¯·æ±‚IDè¿½è¸ª')
    parser.add_argument('--analyze-errors', '-e', action='store_true', help='åˆ†æé”™è¯¯æ—¥å¿—')
    parser.add_argument('--analyze-performance', '-p', action='store_true', help='åˆ†ææ€§èƒ½æ—¥å¿—')
    parser.add_argument('--analyze-slow', action='store_true', help='åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—')
    parser.add_argument('--list', '-l', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--log-file', default='all.log', help='æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼ˆé»˜è®¤all.logï¼‰')
    parser.add_argument('--hours', type=int, default=24, help='åˆ†ææœ€è¿‘Nå°æ—¶ï¼ˆé»˜è®¤24ï¼‰')
    parser.add_argument('--context', '-C', type=int, default=0, help='æ˜¾ç¤ºä¸Šä¸‹æ–‡è¡Œæ•°')

    args = parser.parse_args()

    analyzer = LogAnalyzer()

    # æ‰§è¡Œæ“ä½œ
    if args.list:
        analyzer.list_log_files()
    elif args.tail:
        analyzer.tail_logs(args.log_file, args.lines, args.follow)
    elif args.search:
        analyzer.search_logs(args.search, args.log_file, context=args.context)
    elif args.request_id:
        analyzer.search_by_request_id(args.request_id, args.log_file)
    elif args.analyze_errors:
        analyzer.analyze_errors('error.log', args.hours)
    elif args.analyze_performance:
        analyzer.analyze_performance('performance.log', args.hours)
    elif args.analyze_slow:
        analyzer.analyze_slow_queries('slow.log')
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()


if __name__ == '__main__':
    main()
